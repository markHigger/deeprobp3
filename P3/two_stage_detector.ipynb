{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "DDJwQPZcupab",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# ROB 498-004/599-004 Assignment 3-2: Two-Stage Object Detector - Faster R-CNN\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Anthony OPIPARI, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "JSXasOiouZdl",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Answer:**   \n",
    "Firstname Lastname, #UMID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "PoRTyUc94S1a",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Faster R-CNN: A Classic Two-Stage Anchor-Based Object Detector\n",
    "\n",
    "In this exercise you will implement a **two-stage** object detector, based on [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf), which consists of two modules - Region Proposal Networks (RPN) and Fast R-CNN.\n",
    "We will train it to detect a set of object classes and evaluate the detection accuracy using the classic metric mean Average Precision ([mAP](https://github.com/Cartucho/mAP))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "LfBk3NtRgqaV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "ubB_0e-UAOVK",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "ASkY27ZtA7Is",
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "MzqbYcKdz6ew",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23394,
     "status": "ok",
     "timestamp": 1739733492479,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "HzRdJ3uhe1CR",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "1b61dca0-c7d0-4bba-c8d8-3fda46f0b80a",
    "run_control": {
     "read_only": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "running_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if running_colab:\n",
    "    from google.colab import drive\n",
    "    print('Running on Colab')\n",
    "    drive.mount('/content/drive/', force_remount=True)\n",
    "else:\n",
    "    print('Running locally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "OvUDZWGU3VLV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the following cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "[\"convolutional_networks.ipynb\", \"two_stage_detector_faster_rcnn.ipynb\", \"rob599\", \"convolutional_networks.py\", \"two_stage_detector.py\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739733492479,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "RrAX9FOLpr9k",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "ad323cdb-f496-4518-8e28-f569c1c51605",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2025WN folder and put all the files under P3 folder, then \"2025WN/P3\"\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2025WN/P3'\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "\n",
    "# Add to sys so we can import .py files.\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "RldDumJE48pv",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from two_stage_detector.py!\n",
    "Hello from p3_helper.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `two_stage_detector.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16259,
     "status": "ok",
     "timestamp": 1739733564952,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "pTIwSpkS495_",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "a2245565-e53d-4c40-98c1-b2c9211aeb43",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "time.tzset()\n",
    "\n",
    "from two_stage_detector import hello_two_stage_detector\n",
    "from rob599.p3_helper import hello_helper\n",
    "\n",
    "\n",
    "hello_two_stage_detector()\n",
    "hello_helper()\n",
    "\n",
    "two_stage_detector_path = os.path.join(GOOGLE_DRIVE_PATH, \"two_stage_detector.py\")\n",
    "two_stage_detector_edit_time = time.ctime(\n",
    "    os.path.getmtime(two_stage_detector_path)\n",
    ")\n",
    "print(\"two_stage_detector.py last edited on %s\" % two_stage_detector_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "GWP1vCGL5Eca",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Load several useful packages that are used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "CwVZ26yM5G8U",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from rob599.p3_helper import *\n",
    "from rob599 import reset_seed\n",
    "from rob599.grad import rel_error\n",
    "\n",
    "# for plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "# To download the dataset\n",
    "!pip install wget\n",
    "\n",
    "# for mAP evaluation\n",
    "!rm -rf mAP\n",
    "!git clone https://github.com/Cartucho/mAP.git\n",
    "!rm -rf mAP/input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "x7poKGI35JZY",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "Vw3wIuCu5LnU",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "MjJ3uyYBg3Lw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Load PROPS Detection Dataset\n",
    "During the majority of our homework assignments so far, we have used the PROPS Classification dataset for image classification tasks.\n",
    "\n",
    "In order to train and evaluate object detection models, we need a dataset where each image is annotated with a *set* of *bounding boxes*, where each box gives the category label and spatial extent of some object in the image.\n",
    "\n",
    "We will use the [PROPS Detection](https://deeprob.org/datasets/props-detection/) dataset, which provides annotations of this form. This dataset is inspired in part on the [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) dataset, which ran a series of yearly computer vision competitions from 2005 to 2012, predating the ImageNet challenge which we have discussed in class.\n",
    "The data from the 2007 challenge used to be one of the most popular datasets for evaluating object detection.\n",
    "Our PROPS Detection dataset is much smaller than more recent object detection datasets such as [COCO](http://cocodataset.org/#home), and thus easier to manage in an homework assignment.\n",
    "PROPS comprises annotated bounding boxes for 10 object classes:\n",
    "`[\"master_chef_can\", \"cracker_box\", \"sugar_box\", \"tomato_soup_can\", \"mustard_bottle\", \"tuna_fish_can\", \"gelatin_box\", \"potted_meat_can\", \"mug\", \"large_marker\"]`.\n",
    "The choice of these objects is inspired by the [YCB object and Model set](https://ieeexplore.ieee.org/document/7251504) commonly used in robotic perception models.\n",
    "\n",
    "We create a [`PyTorch Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class\n",
    "named `PROPSDetectionDataset` in `p3_helper.py` that will download the PROPS Detection dataset.\n",
    "This class returns annotations for each image as a nested set of dictionary objects.\n",
    "\n",
    "Run the following two cells to set a few config parameters and then download the train/val sets for the PROPS Detection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AetPkU4wEmlm"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Set a few constants related to data loading.\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "NUM_WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJe14F-D5Cgk"
   },
   "outputs": [],
   "source": [
    "from rob599.p3_helper import PROPSDetectionDataset\n",
    "\n",
    "# NOTE: Set `download=True` for the first time when you set up Google Drive folder.\n",
    "# Turn it back to `False` later for faster execution in the future.\n",
    "# If this hangs, download and place data in your drive manually.\n",
    "train_dataset = PROPSDetectionDataset(\n",
    "    GOOGLE_DRIVE_PATH, \"train\", image_size=IMAGE_SHAPE[0],\n",
    "    download=False  # True (for the first time)\n",
    ")\n",
    "val_dataset = PROPSDetectionDataset(GOOGLE_DRIVE_PATH, \"val\", image_size=IMAGE_SHAPE[0])\n",
    "\n",
    "print(f\"Dataset sizes: train ({len(train_dataset)}), val ({len(val_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwyDJK6kEmlo"
   },
   "source": [
    "Now we wrap these dataset objects with PyTorch dataloaders. The format of output batches will also be same as what you have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bew4ooeAEmlo"
   },
   "outputs": [],
   "source": [
    "# `pin_memory` speeds up CPU-GPU batch transfer, `num_workers=NUM_WORKERS` loads data\n",
    "# on the main CPU process, suitable for Colab.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Use batch_size = 1 during inference - during inference we do not center crop\n",
    "# the image to detect all objects, hence they may be of different size. It is\n",
    "# easier and less redundant to use batch_size=1 rather than zero-padding images.\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "train_loader_iter = iter(train_loader)\n",
    "image_paths, images, gt_boxes = next(train_loader_iter)\n",
    "\n",
    "print(f\"image paths           : {image_paths}\")\n",
    "print(f\"image batch has shape : {images.shape}\")\n",
    "print(f\"gt_boxes has shape    : {gt_boxes.shape}\")\n",
    "\n",
    "print(f\"Five boxes per image  :\")\n",
    "print(gt_boxes[:, :5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "X4WmocEyiXWa",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Visualize PROPS Detection\n",
    "\n",
    "We will visualize a few images and their GT boxes, just to make sure that everything is loaded properly. After running the following cell, you should see output consisting of three images plotted with the corresponding object bounding boxes for each visible class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "ld1s28Z4fyL5",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from rob599.utils import detection_visualizer\n",
    "\n",
    "inverse_norm = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0., 0., 0.], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
    "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for idx, (_, image, gt_boxes) in enumerate(train_dataset):\n",
    "    if idx > 2:\n",
    "        break\n",
    "\n",
    "    image = inverse_norm(image)\n",
    "    is_valid = gt_boxes[:, 4] >= 0\n",
    "    detection_visualizer(image, val_dataset.idx_to_class, gt_boxes[is_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRDod4f3c11N"
   },
   "source": [
    "## Implementing Backbone and Feature Pyramid Network\n",
    "\n",
    "First, we start building the backbone and FPN of our detector. It is the core component that takes in an image and outputs features of different scales. It can be any type of convolutional network that progressively downsamples the image (e.g. via intermediate max pooling).\n",
    "\n",
    "Here, we use a small [RegNetX-400MF](https://pytorch.org/vision/stable/models.html#torchvision.models.regnet_x_400mf) as the backbone so we can train in reasonable time on Colab. We have already implemented the minimal logic to initialize this backbone from pre-trained ImageNet weights and extract intermediate features `(c3, c4, c5)`.\n",
    "These features `(c3, c4, c5)` have height and width that is ${1/8}^{th}$, ${1/16}^{th}$, and ${1/32}^{th}$ of the input image respectively.\n",
    "These values `(8, 16, 32)` are called the \"stride\" of these features.\n",
    "In other words, it means that moving one location on the FPN level is equivalent to moving `stride` pixels in the input image.\n",
    "\n",
    "FPN will convert these `(c3, c4, c5)` multi-scale features to `(p3, p4, p5)`. These notations \"p3\", \"p4\", \"p5\" are called _FPN levels_.\n",
    "Before you write any code, let's initialize the backbone in the next cell. You should see the shape of `(c3, c4, c5)` features for an input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3BaYo36ddPr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from two_stage_detector import DetectorBackboneWithFPN\n",
    "\n",
    "backbone = DetectorBackboneWithFPN(out_channels=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiZSv5UyeFSY"
   },
   "source": [
    "Follow the instructions in `two_stage_detector.py` to implement additional FPN layers in the `DetectorBackboneWithFPN` class for transforming `(c3, c4, c5)` to `(p3, p4, p5)`.\n",
    "For training a small enough model on Google Colab, we leave out `(p6, p7)`.\n",
    "Output features from these FPN levels are expected to have same height and width as backbone features, but now they should have the same number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erukPwM-ew9A"
   },
   "outputs": [],
   "source": [
    "print(\"Extra FPN modules added:\")\n",
    "print(backbone.fpn_params)\n",
    "\n",
    "# Pass a batch of dummy images (random tensors) in NCHW format and observe the output.\n",
    "dummy_images = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "# Collect dummy output.\n",
    "dummy_fpn_feats = backbone(dummy_images)\n",
    "\n",
    "print(f\"For dummy input images with shape: {dummy_images.shape}\")\n",
    "for level_name, feat in dummy_fpn_feats.items():\n",
    "    print(f\"Shape of {level_name} features: {feat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "oRc7P-RvRZGZ",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Faster R-CNN first stage: Region Proposal Network (RPN)\n",
    "\n",
    "We will now implement the first-stage of Faster R-CNN. It comprises a **Region Proposal Network (RPN)** that learns to predict general _object proposals_, which will then be used by the second stage to make final predictions.\n",
    "\n",
    "**RPN prediction:** An input image is passed through the backbone and we obtain its FPN feature maps `(p3, p4, p5)`.\n",
    "The RPN predicts multiple values at _every location on FPN features_. Faster R-CNN is _anchor-based_ — the model assumes that every location has multiple pre-defined boxes (called \"anchors\") and it predicts two measures per anchor, per FPN location:\n",
    "\n",
    "1. **Objectness:** The likelihood of having _any_ object inside the anchor. This is _class-agnostic_: it only performs binary foreground/background classification.\n",
    "2. **Box regression deltas:** 4-D \"deltas\" that _transform_ an anchor at that location to a ground-truth box.\n",
    "\n",
    "![pred_scores2](https://miro.medium.com/max/918/1*wB3ctS9WGNmw6pP_kjLjgg.png)\n",
    "\n",
    "**SIDE NOTE:** We will use the more common practice of predicting `k` logits and use a logistic regressor instead of `2k` scores (and 2-way softmax) as shown in the above Figure. This slightly reduces the number of trainable parameters.\n",
    "\n",
    "Now follow the instructions in `RPNPredictionNetwork` of `two_stage_detector.py` and implement layers to predict objectness and box regression deltas.\n",
    "Execute the following cell to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXcvPbqKEmls"
   },
   "outputs": [],
   "source": [
    "from two_stage_detector import RPNPredictionNetwork\n",
    "\n",
    "\n",
    "rpn_pred_net = RPNPredictionNetwork(\n",
    "    in_channels=64, stem_channels=[64], num_anchors=3\n",
    ")\n",
    "\n",
    "# Pass the dummy FPN feats to RPN prediction network and view its output shapes.\n",
    "dummy_rpn_obj, dummy_rpn_box = rpn_pred_net(dummy_fpn_feats)\n",
    "\n",
    "# Few expected outputs:\n",
    "# Shape of p4 RPN objectness: torch.Size([2, 196, 3])\n",
    "# Shape of p5 RPN box deltas: torch.Size([2, 49, 12])\n",
    "\n",
    "print(f\"\\nFor dummy input images with shape: {dummy_images.shape}\")\n",
    "for level_name in dummy_fpn_feats.keys():\n",
    "    print(f\"Shape of {level_name} FPN features  : {dummy_fpn_feats[level_name].shape}\")\n",
    "    print(f\"Shape of {level_name} RPN objectness: {dummy_rpn_obj[level_name].shape}\")\n",
    "    print(f\"Shape of {level_name} RPN box deltas: {dummy_rpn_box[level_name].shape}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQG6oYN-f_1K"
   },
   "source": [
    "## Assigning a GT target to every model anchor\n",
    "\n",
    "Faster R-CNN makes predictions at every anchor.\n",
    "We need to assign a GT target for each anchor during training. So we can view this problem as assigning GT boxes (and their class labels) to every FPN feature map location.\n",
    "\n",
    "GT boxes are available (from the dataloader) as 5D vectors `(x1, y1, x2, y2, C)` where `(x1, y1)` is the top-left co-ordinate and `(x2, y2)` is the bottom-right co-ordinate of the bounding box, and `C` is its object class label. These co-ordinates are absolute and real-valued in image dimensions. To begin with the assignment, we will represent every location on an FPN level with `(xc, yc)` absolute and real-valued co-ordinates of a point on the image, that are centers of the receptive fields of those features.\n",
    "\n",
    "For example, given features from FPN level having shape `(batch_size, channels, H / stride, W / stride)` and the location `feature[:, :, i, j]` will map to the image pixel `(stride * (i + 0.5), stride * (j + 0.5))` - 0.5 indicates the shift from top-left corner to the center of \"stride box\".\n",
    "\n",
    "Implement the `get_fpn_location_coords` in `two_stage_detector.py` to get `(xc, yc)` location co-ordinates of all FPN features. Follow its documentation and see its usage example in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TO1rULgOf1-w"
   },
   "outputs": [],
   "source": [
    "from two_stage_detector import get_fpn_location_coords\n",
    "\n",
    "# Get shapes of each FPN level feature map. We don't call these \"dummy\" because\n",
    "# they don't depend on the _values_ of features, but rather only shapes.\n",
    "fpn_feats_shapes = {\n",
    "    level_name: feat.shape for level_name, feat in dummy_fpn_feats.items()\n",
    "}\n",
    "\n",
    "# Get CPU tensors for this sanity check: (you can pass `device=` argument.\n",
    "locations_per_fpn_level = get_fpn_location_coords(fpn_feats_shapes, backbone.fpn_strides)\n",
    "\n",
    "# First five location co-ordinates for each feature maps.\n",
    "expected_locations = {\n",
    "    \"p3\": torch.tensor([[4.0, 4.0], [4.0, 12.0], [4.0, 20.0], [4.0, 28.0], [4.0, 36.0]]),\n",
    "    \"p4\": torch.tensor([[8.0, 8.0], [8.0, 24.0], [8.0, 40.0], [8.0, 56.0], [8.0, 72.0]]),\n",
    "    \"p5\": torch.tensor([[16.0, 16.0], [16.0, 48.0], [16.0, 80.0], [16.0, 112.0], [16.0, 144.0]]),\n",
    "}\n",
    "\n",
    "print(\"First five locations per FPN level (absolute image co-ordinates):\")\n",
    "for level_name, locations in locations_per_fpn_level.items():\n",
    "    print(f\"{level_name}: {locations[:5, :].tolist()}\")\n",
    "    print(\"rel error: \", rel_error(expected_locations[level_name], locations[:5, :]))\n",
    "\n",
    "# Visualize all the locations on first image from training data.\n",
    "for level_name, locations in locations_per_fpn_level.items():\n",
    "    # Un-normalize image to bring in [0, 1] RGB range.\n",
    "    image = inverse_norm(val_dataset[0][1])\n",
    "\n",
    "    print(\"*\" * 80)\n",
    "    print(f\"All locations of the image FPN level = {level_name}\")\n",
    "    print(f\"stride = {backbone.fpn_strides[level_name]}\")\n",
    "    detection_visualizer(image, val_dataset.idx_to_class, points=locations.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "etBYc7rbj35F",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Anchor-based Training of RPN\n",
    "\n",
    "Now that we implemented this RPN head, our goal is to have it predict _high objectness_ and _accurate box deltas_ for anchors that are likely to contain objects.\n",
    "We need to assign a target GT box to every RPN prediction for training supervision.\n",
    "\n",
    "**Faster R-CNN is anchor-based:** instead of _all possible locations_, it makes predictions with reference to some pre-defined _anchor boxes_, and matches each anchor with a single GT box if they have a high enough Intersection-over-Union (IoU).\n",
    "\n",
    "In the next few cells, we will perform the following steps:\n",
    "\n",
    "1. **Anchor generation:** Generate a set of anchors for every location in FPN feature map.\n",
    "2. **Anchor to GT matching:** Match these anchors with GT boxes based on their IoU-overlap.\n",
    "3. **Format of box deltas:** Implement the tranformation functions to obtain _box deltas_ from GT boxes (model training supervision) and apply deltas to anchors (final proposal boxes for second stage).\n",
    "\n",
    "Let's approach these steps, one at a time.\n",
    "\n",
    "### Anchor Generation\n",
    "\n",
    "Recall that you now have implemented a function to get the absolute image co-ordinates of FPN feature map locations — in `get_fpn_location_coords`.\n",
    "First we need to form multiple anchor boxes centered at these locations.\n",
    "RPN defines square anchor boxes of size `scale * stride` at every location, where `stride` is the FPN level stride, and `scale` is a hyperparameter.\n",
    "For example, anchor boxes for P5 level (`stride = 32`), with `scale = 2` will be boxes of `(64 x 64)` pixels.\n",
    "RPN also considers anchors of different aspect ratios, apart from square anchor boxes —\n",
    "follow the instructions in `generate_fpn_anchors` of `two_stage_detector.py` to implement creation of multiple anchor boxes per location.\n",
    "\n",
    "Execute the next cell to verify your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "O5w-EUJekJj-",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from two_stage_detector import get_fpn_location_coords, generate_fpn_anchors\n",
    "\n",
    "\n",
    "# Sanity check: Get 2x2 location co-ordinates of p5 (original shape is 7x7).\n",
    "locations = get_fpn_location_coords(\n",
    "    shape_per_fpn_level={\"p5\": (2, 64, 2, 2)}, strides_per_fpn_level={\"p5\": 32}\n",
    ")\n",
    "\n",
    "print(\"P5 locations:\\n\", locations[\"p5\"])\n",
    "\n",
    "# Generate anchors for these locations.\n",
    "anchors = generate_fpn_anchors(\n",
    "    locations_per_fpn_level=locations,\n",
    "    strides_per_fpn_level={\"p5\": 32},\n",
    "    stride_scale=2,\n",
    "    aspect_ratios=[0.5, 1.0, 2.0],\n",
    ")\n",
    "\n",
    "print(\"P5 anchors with different aspect ratios:\")\n",
    "print(\"P5 1:2 anchors:\\n\", anchors[\"p5\"][0::3, :], \"\\n\")\n",
    "# Expected (any ordering is fine):\n",
    "# [-29.2548,  -6.6274,  61.2548,  38.6274]\n",
    "# [-29.2548,  25.3726,  61.2548,  70.6274]\n",
    "# [  2.7452,  -6.6274,  93.2548,  38.6274]\n",
    "# [  2.7452,  25.3726,  93.2548,  70.6274]\n",
    "\n",
    "print(\"P5 1:1 anchors:\\n\", anchors[\"p5\"][1::3, :], \"\\n\")\n",
    "# Expected (any ordering is fine):\n",
    "# [-16., -16.,  48.,  48.]\n",
    "# [-16.,  16.,  48.,  80.]\n",
    "# [ 16., -16.,  80.,  48.]\n",
    "# [ 16.,  16.,  80.,  80.]\n",
    "\n",
    "print(\"P5 2:1 anchors:\\n\", anchors[\"p5\"][2::3, :], \"\\n\")\n",
    "# Similar to 1:2 anchors, but with flipped co-ordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WLcvSDREmlu"
   },
   "source": [
    "### Matching anchor boxes with GT boxes\n",
    "\n",
    "We will now match these generated anchors with GT boxes. Faster R-CNN matches some `N` anchor boxes with `M` GT boxes by applying a simple rule:\n",
    "\n",
    "> Anchor box $N_i$ is matched with box $M_i$ if it has an IoU overlap higher than 0.6 with that box. For multiple such GT boxes, the anchor is assigned with the GT box that has the highest IoU. Note that a single ground-truth box may assign positive labels to multiple anchors.\n",
    "\n",
    "**NOTE:** _Faster R-CNN uses 0.7 default threshold_ as mentioned in the lecture slides. We use a lower threeshold to increase the number of positive matches for sampling — this helps in speeding up training in a resource constrained setting like Google Colab.\n",
    "\n",
    "Anchor boxes with `IoU < 0.3` with ALL GT boxes is assigned background GT box `(-1, -1, -1, -1, -1)`. All other anchors with IoU between `(0.3, 0.6)` are considered \"neutral\" and ignored during training. It is worth noting that the \"neutral\" Faster R-CNN anchors cause wasted computation, and removing this redundancy would overly complicate our implementation.\n",
    "\n",
    "We have implemented this matching procedure for you — see `rcnn_match_anchors_to_gt` of `two_stage_detector.py`.\n",
    "Read its documentation to understand its input/output format.\n",
    "It serves to define GT targets for model predictions during training.\n",
    "\n",
    "This function internally requires IoU computation between all anchors and GT boxes — which you have to implement.\n",
    "Follow the instructions in `two_stage_detector.py` to implement IoU computation, and execute the next cell for a sanity check — you should observe an error of `1e-7` or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "fK_USCuaXSzh",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from two_stage_detector import iou\n",
    "\n",
    "\n",
    "boxes1 = torch.Tensor([[10, 10, 90, 90], [20, 20, 40, 40], [60, 60, 80, 80]])\n",
    "boxes2 = torch.Tensor([[10, 10, 90, 90], [60, 60, 80, 80], [30, 30, 70, 70]])\n",
    "\n",
    "expected_iou = torch.Tensor(\n",
    "    [[1.0, 0.0625, 0.25], [0.0625, 0.0, 0.052631579], [0.0625, 1.0, 0.052631579]]\n",
    ")\n",
    "result_iou = iou(boxes1, boxes2)\n",
    "\n",
    "print(\"Relative error:\", rel_error(expected_iou, result_iou))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHocRu-zEmlv"
   },
   "source": [
    "### Visualizing matched GT boxes\n",
    "\n",
    "Now we apply our anchor matching function and visualize one GT box with a random matched positive anchor box.\n",
    "You may try different images by indexing `train_dataset` below,\n",
    "make sure to try different FPN levels as certain images may not get any matched GT boxes due to their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "wPvX4TrgaLD8",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from two_stage_detector import get_fpn_location_coords, generate_fpn_anchors, rcnn_match_anchors_to_gt\n",
    "\n",
    "\n",
    "# Sanity check: Match anchors of p3 level with GT boxes of first image\n",
    "# in the training dataset.\n",
    "_, image, gt_boxes = train_dataset[0]\n",
    "\n",
    "FPN_LEVEL = \"p3\"\n",
    "FPN_STRIDE = 8\n",
    "locations = get_fpn_location_coords(\n",
    "    shape_per_fpn_level={FPN_LEVEL: (2, 64, 224 // FPN_STRIDE, 224 // FPN_STRIDE)},\n",
    "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE}\n",
    ")\n",
    "# Generate anchors for these locations.\n",
    "anchors = generate_fpn_anchors(\n",
    "    locations_per_fpn_level=locations,\n",
    "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE},\n",
    "    stride_scale=8,  # Default value used by Faster R-CNN\n",
    "    aspect_ratios=[0.5, 1.0, 2.0],\n",
    ")\n",
    "\n",
    "matched_gt_boxes = rcnn_match_anchors_to_gt(\n",
    "    anchors[FPN_LEVEL], gt_boxes, iou_thresholds=(0.3, 0.6)\n",
    ")\n",
    "\n",
    "# Flatten anchors and matched boxes:\n",
    "anchors_p4 = anchors[FPN_LEVEL].view(-1, 4)\n",
    "matched_boxes_p4 = matched_gt_boxes.view(-1, 5)\n",
    "\n",
    "# Visualize one selected anchor and its matched GT box.\n",
    "# NOTE: Run this cell multiple times to see different matched anchors. For car\n",
    "# image, p3/5 will not work because the GT box was already assigned to p4.\n",
    "fg_idxs_p4 = (matched_boxes_p4[:, 4] > 0).nonzero()\n",
    "fg_idx = random.choice(fg_idxs_p4)\n",
    "\n",
    "# Combine both boxes for visualization:\n",
    "dummy_vis_boxes = [anchors_p4[fg_idx][0], matched_boxes_p4[fg_idx][0]]\n",
    "\n",
    "print(\"Unlabeled red box is positive anchor:\")\n",
    "detection_visualizer(\n",
    "    inverse_norm(image),\n",
    "    val_dataset.idx_to_class,\n",
    "    bbox=dummy_vis_boxes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "XW_Zek3_dgfF",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### GT Targets for box regression\n",
    "\n",
    "Now we work on the third and final component needed to train our RPN — we define transformation functions for box regression deltas. You will find these transforms in [Lecture 12](https://deeprob.org/calendar/#lec-12), follow these and implement two functions in `two_stage_detector.py`:\n",
    "\n",
    "1. `rcnn_get_deltas_from_anchors`: Accepts anchor boxes and GT boxes, and returns deltas. Required for training supervision.\n",
    "2. `rcnn_apply_deltas_to_anchors`: Accepts predicted deltas and anchor boxes, and returns predicted boxes. Required during inference.\n",
    "\n",
    "Run the following cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "MX2JCaOf0768",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from two_stage_detector import rcnn_get_deltas_from_anchors, rcnn_apply_deltas_to_anchors\n",
    "\n",
    "# Three hard-coded anchor boxes and GT boxes that have a fairly high overlap.\n",
    "# Add a dummy class ID = 1 indicating foreground\n",
    "input_anchors = torch.Tensor(\n",
    "    [[20, 40, 80, 90], [10, 10, 50, 50], [120, 100, 200, 200]]\n",
    ")\n",
    "input_boxes = torch.Tensor(\n",
    "    [[10, 15, 100, 115, 1], [30, 20, 40, 30, 1], [120, 100, 200, 200, 1]]\n",
    ")\n",
    "\n",
    "# Here we do a simple sanity check - getting deltas for a particular set of boxes\n",
    "# and applying them back to anchors should give us the same boxes.\n",
    "_deltas = rcnn_get_deltas_from_anchors(input_anchors, input_boxes)\n",
    "output_boxes = rcnn_apply_deltas_to_anchors(_deltas, input_anchors)\n",
    "\n",
    "print(\"Rel error in reconstructed boxes:\", rel_error(input_boxes[:, :4], output_boxes))\n",
    "\n",
    "# Another check: deltas for GT class label = -1 should be -1e8\n",
    "background_box = torch.Tensor([[-1, -1, -1, -1, -1]])\n",
    "input_anchor = torch.Tensor([[100, 100, 200, 200]])\n",
    "\n",
    "_deltas = rcnn_get_deltas_from_anchors(input_anchor, background_box)\n",
    "output_box = rcnn_apply_deltas_to_anchors(_deltas, input_anchor)\n",
    "\n",
    "print(\"Background deltas should be all -1e8  :\", _deltas)\n",
    "print(\"Output box should be -1e8 or lower    :\", output_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "dlO2IUCnt4zu",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "With all predictions assigned with GT targets, we will proceed to compute losses for training the RPN. Here, you will use L1 loss for box regression.\n",
    "\n",
    "**Objectness classification loss:** Focal Loss was proposed in RetinaNet (2017) to deal with heavy class imbalance caused by \"background\". Faster R-CNN predates this paper — it dealt with class imbalance by randomly sampling roughly equal amount of foreground-background anchors to train RPN. We have implemented a very simple sampling function for you in `sample_rpn_training` function of `two_stage_detector.py` — you may directly use it while you piece all these components (coming up next).\n",
    "\n",
    "**Total loss** is the sum of both loss components _per sampled anchor_, averaged by total number of foreground + background anchors.\n",
    "\n",
    "Execute the next cell to quickly recap their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "2eSleGX9yTeo",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# Sanity check: dummy predictions from model - box regression deltas and\n",
    "# objectness logits for two anchors.\n",
    "# shape: (batch_size, HWA, 4 or 1)\n",
    "dummy_pred_boxreg_deltas = torch.randn(1, 2, 4)\n",
    "dummy_pred_obj_logits = torch.randn(1, 2, 1)\n",
    "\n",
    "# Dummy deltas and objectness targets. Let the second box be background.\n",
    "# Dummy GT boxes (matched with both anchors).\n",
    "dummy_gt_deltas = torch.randn_like(dummy_pred_boxreg_deltas)\n",
    "dummy_gt_deltas[:, 1, :] = -1e8\n",
    "\n",
    "# Background objectness targets should be 0 (not -1), and foreground\n",
    "# should be 1. Neutral anchors will not occur here due to sampling.\n",
    "dummy_gt_objectness = torch.Tensor([1, 0])\n",
    "\n",
    "# Note that loss is not multiplied with 0.25 here:\n",
    "loss_box = F.l1_loss(\n",
    "    dummy_pred_boxreg_deltas, dummy_gt_deltas, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# No loss for background anchors:\n",
    "loss_box[dummy_gt_deltas == -1e8] *= 0.0\n",
    "print(\"Box regression loss (L1):\", loss_box)\n",
    "\n",
    "# Now calculate objectness loss.\n",
    "loss_obj = F.binary_cross_entropy_with_logits(\n",
    "    dummy_pred_obj_logits.view(-1), dummy_gt_objectness, reduction=\"none\"\n",
    ")\n",
    "print(\"Objectness loss (BCE):\", loss_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09gh9SCllz-b"
   },
   "source": [
    "## Non-Maximum Suppression (NMS)\n",
    "\n",
    "The definition of NMS and instructions on how to compute NMS can be found in the lecture slides:\n",
    "https://deeprob.org/calendar/#lec-12\n",
    "\n",
    "Implement the `nms` function in `two_stage_detector.py`. We then compare your implementation of NMS with the implementation in torchvision. Most likely, your implementation will be faster on CPU than on CUDA, and the torchvision implementation will likely be much faster than yours.\n",
    "This is expected, but your implementation should produce the same outputs as the torchvision version.\n",
    "\n",
    "This utility will be used for building Faster R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8xOxkcSmAZT"
   },
   "outputs": [],
   "source": [
    "# Perform imports here to make this cell runnble independently,\n",
    "# students are likely to spend good mount of time here and it is\n",
    "# best to not require execution of prior cells.\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from two_stage_detector import nms\n",
    "from rob599 import reset_seed\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "\n",
    "boxes = (100.0 * torch.rand(5000, 4)).round()\n",
    "boxes[:, 2] = boxes[:, 2] + boxes[:, 0] + 1.0\n",
    "boxes[:, 3] = boxes[:, 3] + boxes[:, 1] + 1.0\n",
    "scores = torch.randn(5000)\n",
    "\n",
    "names = [\"your_cpu\", \"torchvision_cpu\", \"torchvision_cuda\"]\n",
    "iou_thresholds = [0.3, 0.5, 0.7]\n",
    "elapsed = dict(zip(names, [0.0] * len(names)))\n",
    "intersects = dict(zip(names[1:], [0.0] * (len(names) - 1)))\n",
    "\n",
    "for iou_threshold in iou_thresholds:\n",
    "    tic = time.time()\n",
    "    my_keep = nms(boxes, scores, iou_threshold)\n",
    "    elapsed[\"your_cpu\"] += time.time() - tic\n",
    "\n",
    "    tic = time.time()\n",
    "    tv_keep = torchvision.ops.nms(boxes, scores, iou_threshold)\n",
    "    elapsed[\"torchvision_cpu\"] += time.time() - tic\n",
    "    intersect = len(set(tv_keep.tolist()).intersection(my_keep.tolist())) / len(tv_keep)\n",
    "    intersects[\"torchvision_cpu\"] += intersect\n",
    "\n",
    "    tic = time.time()\n",
    "    tv_cuda_keep = torchvision.ops.nms(boxes.to(device=DEVICE), scores.to(device=DEVICE), iou_threshold).to(\n",
    "        my_keep.device\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed[\"torchvision_cuda\"] += time.time() - tic\n",
    "    intersect = len(set(tv_cuda_keep.tolist()).intersection(my_keep.tolist())) / len(\n",
    "        tv_cuda_keep\n",
    "    )\n",
    "    intersects[\"torchvision_cuda\"] += intersect\n",
    "\n",
    "for key in intersects:\n",
    "    intersects[key] /= len(iou_thresholds)\n",
    "\n",
    "# You should see < 1% difference\n",
    "print(\"Testing NMS:\")\n",
    "print(\"Your        CPU  implementation: %fs\" % elapsed[\"your_cpu\"])\n",
    "print(\"torchvision CPU  implementation: %fs\" % elapsed[\"torchvision_cpu\"])\n",
    "print(\"torchvision CUDA implementation: %fs\" % elapsed[\"torchvision_cuda\"])\n",
    "print(\"Speedup CPU : %fx\" % (elapsed[\"your_cpu\"] / elapsed[\"torchvision_cpu\"]))\n",
    "print(\"Speedup CUDA: %fx\" % (elapsed[\"your_cpu\"] / elapsed[\"torchvision_cuda\"]))\n",
    "print(\n",
    "    \"Difference CPU : \", 1.0 - intersects[\"torchvision_cpu\"]\n",
    ")  # in the order of 1e-3 or less\n",
    "print(\n",
    "    \"Difference CUDA: \", 1.0 - intersects[\"torchvision_cuda\"]\n",
    ")  # in the order of 1e-3 or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "AiPfXUHPupDE",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Putting it all together: RPN module\n",
    "\n",
    "Now you will put together all the things you have implemented into the `RPN` class in `two_stage_detector.py`.\n",
    "Implement `forward` and `predict_proposals` functions of this module — you have already done most of the heavy lifting, you simply need to call all the functions in a correct way!\n",
    "Use the previous two cells as a reference to implement loss calculation in `forward()`.\n",
    "\n",
    "## Overfit small data\n",
    "\n",
    "In Faster R-CNN, the RPN is trained jointly with the second-stage network.\n",
    "However, to test our RPN implementation, we will first train just the RPN.\n",
    "We will use the `train_detector` function. You can read its implementation in `rob599/p3_helper.py`.\n",
    "\n",
    "The loss should generally do down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "YTObddiog9wJ",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from rob599.p3_helper import train_detector\n",
    "from two_stage_detector import DetectorBackboneWithFPN, RPN\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# Take equally spaced examples from training dataset to make a subset.\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    train_dataset,\n",
    "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
    ")\n",
    "small_train_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Create a wrapper module to contain backbone + RPN:\n",
    "class FirstStage(nn.Module):\n",
    "    def __init__(self, fpn_channels: int):\n",
    "        super().__init__()\n",
    "        self.backbone = DetectorBackboneWithFPN(out_channels=fpn_channels)\n",
    "        self.rpn = RPN(\n",
    "            fpn_channels=fpn_channels,\n",
    "            # Simple stem of two layers:\n",
    "            stem_channels=[fpn_channels, fpn_channels],\n",
    "            batch_size_per_image=16,\n",
    "            anchor_stride_scale=8,\n",
    "            anchor_aspect_ratios=[0.5, 1.0, 2.0],\n",
    "            anchor_iou_thresholds=(0.3, 0.6),\n",
    "        )\n",
    "\n",
    "    def forward(self, images, gt_boxes=None):\n",
    "        feats_per_fpn_level = self.backbone(images)\n",
    "        return self.rpn(feats_per_fpn_level, self.backbone.fpn_strides, gt_boxes)\n",
    "\n",
    "\n",
    "first_stage = FirstStage(fpn_channels=64).to(DEVICE)\n",
    "\n",
    "train_detector(\n",
    "    first_stage,\n",
    "    small_train_loader,\n",
    "    learning_rate=8e-3,\n",
    "    max_iters=1000,\n",
    "    log_period=20,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "jKjv6JLMRj7s",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Faster R-CNN\n",
    "\n",
    "We have implemented the first half of Faster R-CNN, i.e., RPN, which is class-agnostic. Here, we briefly describe the second half Fast R-CNN.\n",
    "\n",
    "Given a set of proposal boxes from RPN (per FPN level, per image),\n",
    "we warp each region from the correspondng map to a fixed size 7x7 by using [RoI Align](https://arxiv.org/pdf/1703.06870.pdf).\n",
    "We will use the `roi_align` function from `torchvision`. For usage instructions, see https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_align\n",
    "\n",
    "For simplicity and computational constraints of Google Colab,\n",
    "our two-stage detector here differs from a standard Faster R-CNN system in the second stage:\n",
    "In a full implementation, the second stage of the network would predict a box deltas to further refine RPN proposals.\n",
    "We omit this for simplicity and keep RPN proposal boxes as final predictions.\n",
    "Your model will definitely perform better if you add an extra box regression head in second stage.\n",
    "\n",
    "### Your implementation exercise\n",
    "\n",
    "Read `FasterRCNN` class documentation and code to understand how everything is pieced together.\n",
    "By now you have already implemented the core components of a typical object detection system - you have dealt with anchor boxes, matched them with GT boxes, supervised model with your matching, and wrote inference utilities like NMS.\n",
    "Great work!\n",
    "\n",
    "### Classification Loss: cross entropy\n",
    "\n",
    "The classification loss for second-stage is a cross entropy loss — you would have seen this in A3, and it is a multi-class extension of binary cross entropy loss used in RPN objectness classification. You may use `torch.nn.functional.cross_entropy` directly — follow instructions in Python script.\n",
    "\n",
    "Beyond these, the second stage of Faster R-CNN doesn't add anything that is conceptually new — hence your implementation exercise is fairly lightweight.\n",
    "We have implemented most of this module for you. We left out a few 3-4 line TODO blocks, only because if we wrote them, they would given away the solution for prior exercises (RPN).\n",
    "Moreover, empty code blocks will encourage you to carefully read the remaining portions for making everything work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "RFZ49wox4MYn",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Overfit small data\n",
    "\n",
    "After adding your implementation, overfit the model on a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "WbxeAJq0zc3F",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from two_stage_detector import FasterRCNN\n",
    "\n",
    "\n",
    "# Re-initialize dataset objects for independent debugging.\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    train_dataset,\n",
    "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
    ")\n",
    "small_train_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "FPN_CHANNELS = 64\n",
    "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
    "rpn = RPN(\n",
    "    fpn_channels=FPN_CHANNELS,\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=16,\n",
    "    anchor_stride_scale=8,\n",
    "    anchor_aspect_ratios=[0.5, 1.0, 2.0],\n",
    "    anchor_iou_thresholds=(0.3, 0.6),\n",
    "    pre_nms_topk=400,\n",
    "    post_nms_topk=80,\n",
    ")\n",
    "# fmt: off\n",
    "faster_rcnn = FasterRCNN(\n",
    "    backbone, rpn, num_classes=NUM_CLASSES, roi_size=(7, 7),\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=32,\n",
    ")\n",
    "# fmt: on\n",
    "\n",
    "train_detector(\n",
    "    faster_rcnn,\n",
    "    small_train_loader,\n",
    "    learning_rate=0.01,\n",
    "    max_iters=1000,\n",
    "    log_period=10,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "_SWA1DbG47ln",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now, follow the instructions in `FasterRCNN.inference` to implement inference.\n",
    "\n",
    "Visualize the output from the trained model on a few images by executing the next cell, the bounding boxes should be somewhat accurate. They would get even better by using a bigger model and training it for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "gp_Hmt-Km5bl",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from rob599.p3_helper import inference_with_detector\n",
    "\n",
    "\n",
    "# Change the loader to have (batch size = 1) as required for inference.\n",
    "small_train_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "inference_with_detector(\n",
    "    faster_rcnn,\n",
    "    small_train_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.2,\n",
    "    nms_thresh=0.5,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "sr7wNngy4oZf",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Train a net\n",
    "\n",
    "Now it's time to train the full Faster R-CNN model on a larger subset of the the training data.\n",
    "We will train for 9000 iterations; this should take about 2-3 hours on a K80 GPU.\n",
    "Note that real object detection systems typically train for 12-24 hours, distribute training over multiple GPUs, and use much faster GPUs. As such our result will be far from the state of the art, but it should give some reasonable results!\n",
    "\n",
    "(Optional) If you train the model longer (e.g., 25K+ iterations), you should see a better mAP. But make sure you revert the code back for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "X1k1rx1f4sTE",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from rob599.p3_helper import train_detector\n",
    "from two_stage_detector import DetectorBackboneWithFPN, RPN, FasterRCNN\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# Slightly larger detector than in above cell.\n",
    "FPN_CHANNELS = 128\n",
    "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
    "rpn = RPN(\n",
    "    fpn_channels=FPN_CHANNELS,\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=16,\n",
    "    pre_nms_topk=500,\n",
    "    post_nms_topk=200  # Other args from previous cell are default args in RPN.\n",
    ")\n",
    "# fmt: off\n",
    "faster_rcnn = FasterRCNN(\n",
    "    backbone, rpn, num_classes=NUM_CLASSES, roi_size=(7, 7),\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=32,\n",
    ")\n",
    "# fmt: on\n",
    "\n",
    "train_detector(\n",
    "    faster_rcnn,\n",
    "    train_loader,\n",
    "    learning_rate=0.01,\n",
    "    max_iters=9000,\n",
    "    log_period=50,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# After you've trained your model, save the weights for submission.\n",
    "weights_path = os.path.join(GOOGLE_DRIVE_PATH, \"rcnn_detector.pt\")\n",
    "torch.save(faster_rcnn.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "KhWZT-ztEaqm",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Inference\n",
    "\n",
    "VIsualize a few outputs from the full trained model.\n",
    "These could be improved if we used a larger model, trained for greater duration, and if we included box regression in the second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "J7ArGiLTnHta",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Add some imports to run this cell independently of above few cells\n",
    "# (you will need to run first few cells at the top)\n",
    "from rob599.p3_helper import inference_with_detector\n",
    "from two_stage_detector import RPN, FasterRCNN, DetectorBackboneWithFPN\n",
    "\n",
    "\n",
    "# Re-initialize so this cell is independent from prior cells.\n",
    "# Slightly larger detector than in above cell.\n",
    "FPN_CHANNELS = 128\n",
    "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
    "rpn = RPN(fpn_channels=FPN_CHANNELS, stem_channels=[FPN_CHANNELS, FPN_CHANNELS], batch_size_per_image=16)\n",
    "faster_rcnn = FasterRCNN(\n",
    "    backbone, rpn, num_classes=NUM_CLASSES, roi_size=(7, 7),\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=32,\n",
    ")\n",
    "faster_rcnn.to(device=DEVICE)\n",
    "\n",
    "weights_path = os.path.join(GOOGLE_DRIVE_PATH, \"rcnn_detector.pt\")\n",
    "faster_rcnn.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "# Prepare a small val daataset for inference:\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    val_dataset,\n",
    "    torch.linspace(0, len(val_dataset) - 1, steps=20).long()\n",
    ")\n",
    "small_val_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "inference_with_detector(\n",
    "    faster_rcnn,\n",
    "    small_val_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.2,\n",
    "    nms_thresh=0.5,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "ETU6ev7aydIY",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate your Faster R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "FvDb7uwqyhAK",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "inference_with_detector(\n",
    "    faster_rcnn,\n",
    "    val_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.2,\n",
    "    nms_thresh=0.5,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    "    output_dir=\"mAP/input\",\n",
    ")\n",
    "!cd mAP && python main.py\n",
    "\n",
    "# This script outputs an image containing per-class AP. Display it here:\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./mAP/output/mAP.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4L-IaRqXRUc"
   },
   "source": [
    "## Verifying notebook cells\n",
    "\n",
    "Before moving onto the next part of the project, we can verify that no\n",
    "unexpected cells have been added or removed by **saving this file** then using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1739733707533,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "NXg8h5LiXP7S",
    "outputId": "ffdc0ad7-858e-4185-ca9a-81ea8c19950a"
   },
   "outputs": [],
   "source": [
    "from rob599.p3_helper import verify_notebook_cells\n",
    "\n",
    "notebook_path = os.path.join(GOOGLE_DRIVE_PATH, 'two_stage_detector.ipynb')\n",
    "verify_notebook_cells(notebook_path, expected_count=61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "4u69TlnhucrR",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Submit Your Work\n",
    "After completing both notebooks for this assignment (`convolutional_networks.ipynb` and this notebook, `two_stage_detector.ipynb`), run the following cell to create a `.zip` file for you to download and turn in.\n",
    "\n",
    "**Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 50987,
     "status": "error",
     "timestamp": 1739733700238,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 300
    },
    "id": "e6vziUpSuqLY",
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "outputId": "635c3c9c-dfa8-41da-c015-052d77eeb675",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from rob599.submit import make_p3_submission\n",
    "\n",
    "# TODO: Replace these with your actual uniquename and umid\n",
    "uniquename = None\n",
    "umid = None\n",
    "\n",
    "make_p3_submission(GOOGLE_DRIVE_PATH, uniquename, umid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVBvJX_oNNmW"
   },
   "source": [
    "Now you are ready to submit your project to the autograder."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "4ef42baa288ce895b984811292da1481faa2138d6a325169bc8d9d38d49f8a2b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
